{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# SVM Dataset Loader and Evaluator\n\n", "This notebook loads datasets from multiple sources (sklearn, TensorFlow, and Kaggle),\n", "trains an SVM model, and evaluates its performance. The datasets used are:\n\n", "- **Iris Dataset** (from sklearn)\n", "- **MNIST Handwritten Digits Dataset** (from TensorFlow)\n", "- **Titanic Dataset** (from Kaggle)\n\n", "The notebook follows these steps:\n\n", "1. Load datasets from different sources\n", "2. Display 10 samples from each dataset\n", "3. Preprocess and split the data\n", "4. Train an SVM model\n", "5. Make predictions\n", "6. Evaluate model performance\n\n", "## Requirements:\n", "- Ensure you have Kaggle API configured to access datasets.\n", "- Required libraries: `numpy`, `pandas`, `sklearn`, `tensorflow`, `kaggle`.\n\n", "Let's get started! \ud83d\ude80"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Import Necessary Libraries\n\n", "We import the required libraries for data handling, machine learning, and deep learning:\n\n", "- `numpy` and `pandas` for numerical operations and data handling.\n", "- `sklearn` for loading datasets, splitting data, training an SVM model, and evaluating performance.\n", "- `tensorflow` for loading the MNIST dataset.\n", "- `kaggle` API to fetch datasets from Kaggle.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn import datasets\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.svm import SVC\n", "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n", "import tensorflow as tf\n", "from kaggle.api.kaggle_api_extended import KaggleApi\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Load Dataset from sklearn (Iris Dataset)\n\n", "The Iris dataset contains measurements of different species of flowers. \n", "- `datasets.load_iris()` loads the dataset.\n", "- `X` contains the features (sepal length, sepal width, petal length, petal width).\n", "- `y` contains the target labels (flower species: 0, 1, or 2).\n", "- We display 10 samples to understand the dataset structure.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Step 2: Load Dataset from sklearn\n", "iris = datasets.load_iris()\n", "X, y = iris.data, iris.target\n", "print(\"Iris Dataset Sample:\")\n", "print(pd.DataFrame(X).head(10))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Load Dataset from TensorFlow (MNIST Dataset)\n\n", "The MNIST dataset consists of 28x28 grayscale images of handwritten digits (0-9).\n", "- `tf.keras.datasets.mnist.load_data()` loads the dataset.\n", "- `X_train` and `X_test` contain the image data.\n", "- `y_train` and `y_test` contain the corresponding labels (digits).\n", "- Since SVM requires a flat feature representation, we reshape the 28x28 images into 1D arrays.\n", "- We display 10 samples to check the dataset structure.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Step 3: Load Dataset from TensorFlow\n", "mnist = tf.keras.datasets.mnist\n", "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n", "# Flatten the images for SVM\n", "X_train = X_train.reshape((X_train.shape[0], -1))\n", "X_test = X_test.reshape((X_test.shape[0], -1))\n", "print(\"MNIST Dataset Sample:\")\n", "print(pd.DataFrame(X_train).head(10))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Load Dataset from Kaggle (Titanic Dataset)\n\n", "The Titanic dataset contains information about passengers, used to predict survival.\n", "- `KaggleApi()` is used to authenticate and download the dataset.\n", "- The dataset is read using `pd.read_csv()`.\n", "- We drop missing values (`dropna()`) to clean the data.\n", "- `X` contains passenger attributes (age, fare, class, etc.), and `y` contains survival labels (0 or 1).\n", "- We display 10 samples to analyze the dataset.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Step 4: Load Dataset from Kaggle\n", "api = KaggleApi()\n", "api.authenticate()\n", "\n", "# Download a dataset from Kaggle (example: Titanic dataset)\n", "api.dataset_download_files('heptapod/titanic', path='./', unzip=True)\n", "\n", "# Load the dataset into a pandas DataFrame\n", "titanic = pd.read_csv('./titanic/train.csv')\n", "# Preprocess the dataset as needed\n", "titanic = titanic.dropna()  # Example preprocessing step\n", "X = titanic.drop('Survived', axis=1)\n", "y = titanic['Survived']\n", "print(\"Titanic Dataset Sample:\")\n", "print(X.head(10))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Train-Test Split\n\n", "Splitting the dataset into training and testing sets:\n", "- `train_test_split()` randomly divides the data into 70% training and 30% testing.\n", "- This ensures the model is trained on one portion of data and evaluated on another.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Step 5: Train-Test Split (for sklearn and Kaggle datasets)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 6: Train SVM Model\n\n", "We use a Support Vector Machine (SVM) classifier:\n", "- `SVC(kernel='linear')` initializes a linear SVM model.\n", "- `fit(X_train, y_train)` trains the model on the training dataset.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Step 6: Train SVM Model\n", "svm_model = SVC(kernel='linear')\n", "svm_model.fit(X_train, y_train)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 7: Make Predictions\n\n", "We use the trained SVM model to predict outcomes:\n", "- `predict(X_test)` predicts labels for the test dataset.\n", "- These predictions are stored in `y_pred`.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Step 7: Make Predictions\n", "y_pred = svm_model.predict(X_test)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 8: Evaluate the Model\n\n", "We evaluate the model's performance using multiple metrics:\n", "- `accuracy_score(y_test, y_pred)`: Measures overall correctness.\n", "- `precision_score(y_test, y_pred, average='weighted')`: Measures correct positive predictions.\n", "- `recall_score(y_test, y_pred, average='weighted')`: Measures coverage of actual positives.\n", "- `confusion_matrix(y_test, y_pred)`: Displays true positives, false positives, etc.\n", "- The results are printed for analysis.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Step 8: Evaluate the Model\n", "accuracy = accuracy_score(y_test, y_pred)\n", "precision = precision_score(y_test, y_pred, average='weighted')\n", "recall = recall_score(y_test, y_pred, average='weighted')\n", "conf_matrix = confusion_matrix(y_test, y_pred)\n", "\n", "# Print the results\n", "print(f'Accuracy: {accuracy:.2f}')\n", "print(f'Precision: {precision:.2f}')\n", "print(f'Recall: {recall:.2f}')\n", "print('Confusion Matrix:')\n", "print(conf_matrix)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}